resolution: 64
hier: bottom
validation_ratio: 0.004
num_eval_per_epoch: 10
trainer:
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  num_train_epochs: 20
  learning_rate: 0.0008
  weight_decay: 0.001
  fp16: True
  output_dir: ./results_bottom
  evaluation_strategy: steps
  lr_scheduler_type: cosine
  warmup_ratio: 0.1
  logging_dir: ./logs_bottom
  logging_strategy: steps
  save_strategy: epoch
  logging_steps: 10
  gradient_accumulation_steps: 6

model:
  vocab_size: 1026
  num_layers: 6
  num_decoder_layers: 6
  d_ff: 1024
  d_model: 512
  d_kv: 64
  num_heads: 8
  relative_attention_num_buckets: 32
  relative_attention_max_distance: 128
  dropout_rate: 0.1
  pad_token_id: 0
  eos_token_id: 1
  decoder_start_token_id: 0
  n_positions: 4098